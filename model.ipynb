{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Use Case: ZAF043 Crowd Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1 - Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "is_executing": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Frames Shape: (6,)\n",
      "Training Labels Shape: (6,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def load_training_data(directory):\n",
    "    # Initialize lists for frames and labels\n",
    "    frames = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate over the files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Get the full file path\n",
    "        file_path = os.path.join(directory, filename)\n",
    "\n",
    "        # Load the frame using OpenCV\n",
    "        frame = cv2.imread(file_path)\n",
    "\n",
    "        # Preprocess the frame if needed\n",
    "\n",
    "        # Append the frame to the frames list\n",
    "        frames.append(frame)\n",
    "\n",
    "        # Extract the label from the filename (assuming the filename contains the label information)\n",
    "        label = filename.split(\"_\")[0]  # Extract the label from the filename based on the naming convention\n",
    "        labels.append(label)\n",
    "\n",
    "    # Convert frames and labels to numpy arrays\n",
    "    frames = np.array(frames)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return frames, labels\n",
    "\n",
    "# Directory path containing the training data\n",
    "training_data_directory = \"D:\\\\Data Set\\\\Crowd Anomaly Detection\\\\UCSD_Anomaly_Dataset\"\n",
    "\n",
    "# Load training data\n",
    "training_frames, training_labels = load_training_data(training_data_directory)\n",
    "\n",
    "# Print the shape of the training frames and labels\n",
    "print(\"Training Frames Shape:\", training_frames.shape)\n",
    "print(\"Training Labels Shape:\", training_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2 - Data Preprocessing, Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(frames):\n",
    "    features = []\n",
    "    \n",
    "    # Convert frames to grayscale\n",
    "    gray_frames = [cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) for frame in frames]\n",
    "    \n",
    "    # Calculate optical flow for consecutive frames\n",
    "    for i in range(len(frames) - 1):\n",
    "        prev_frame = gray_frames[i]\n",
    "        next_frame = gray_frames[i + 1]\n",
    "        \n",
    "        # Compute optical flow using Lucas-Kanade method\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_frame, next_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "        \n",
    "        # Extract relevant features from the optical flow\n",
    "        # Example: Compute the mean and standard deviation of the flow vectors\n",
    "        mean_flow = np.mean(flow)\n",
    "        std_flow = np.std(flow)\n",
    "        \n",
    "        # Append the extracted features to the list\n",
    "        features.append([mean_flow, std_flow])\n",
    "    \n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3 - Models\n",
    "In this code, the train_mdt_model() function takes features (the extracted features) and num_components (the number of components in the MDT model) as inputs. It initializes a Gaussian Mixture Model (GMM) with n_components=num_components and fits it to the features using the fit() method. The trained model is then returned.\n",
    "The scikit-learn library installed (pip install scikit-learn) before running this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def train_mdt_model(features, num_components):\n",
    "    # Train a Gaussian Mixture Model (GMM) on the extracted features\n",
    "    gmm = GaussianMixture(n_components=num_components)\n",
    "    gmm.fit(features)\n",
    "    return gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_anomaly_score(observation, model):\n",
    "    # Compute the anomaly score by comparing the observation with the learned model\n",
    "    score = -model.score_samples(observation.reshape(1, -1))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frames: 7000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tifffile\n",
    "\n",
    "# Specify the directory path containing the frames\n",
    "directories =[\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train001\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train002\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train003\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train004\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train005\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train006\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train007\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train008\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train009\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train010\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train011\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train012\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train013\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train014\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train015\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train016\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train017\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train018\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train019\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train020\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train020\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train021\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train022\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train023\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train024\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train025\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train026\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train027\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train028\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train029\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train030\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train031\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train032\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train033\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train034\"\n",
    "    \n",
    "]\n",
    "# Initialize an empty list to store the frames\n",
    "frames = []\n",
    "\n",
    "# Iterate over the directories\n",
    "for directory_path in directories:\n",
    "    # Iterate over the files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "        # Check if the file is a .tif image\n",
    "        if filename.endswith(\".tif\"):\n",
    "            # Read the frame using tifffile\n",
    "            frame = tifffile.imread(file_path)\n",
    "\n",
    "            # Append the frame to the list\n",
    "            frames.append(frame)\n",
    "\n",
    "# Print the number of frames loaded\n",
    "print(\"Number of frames:\", len(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "num_components = 5  # Number of components in the MDT model\n",
    "features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(frames):\n",
    "    features = []\n",
    "\n",
    "    # Calculate optical flow for consecutive frames\n",
    "    for i in range(len(frames) - 1):\n",
    "        frame1 = frames[i]\n",
    "        frame2 = frames[i + 1]\n",
    "\n",
    "        # Calculate optical flow\n",
    "        flow = cv2.calcOpticalFlowFarneback(frame1, frame2, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "        # Flatten the flow matrix and append it to the features\n",
    "        flow_flat = flow.reshape(-1)\n",
    "        features.append(flow_flat)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the features\n",
    "features = []\n",
    "\n",
    "# Iterate over the frames and extract features\n",
    "for frame in frames:\n",
    "    # Preprocess frame if needed\n",
    "    # Extract features from the frame\n",
    "    frame_features = extract_features(frame)\n",
    "    features.append(frame_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features to numpy array\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [42]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Train MDT model\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m mdt_model \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_mdt_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_components\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36mtrain_mdt_model\u001B[1;34m(features, num_components)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_mdt_model\u001B[39m(features, num_components):\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;66;03m# Train a Gaussian Mixture Model (GMM) on the extracted features\u001B[39;00m\n\u001B[0;32m      5\u001B[0m     gmm \u001B[38;5;241m=\u001B[39m GaussianMixture(n_components\u001B[38;5;241m=\u001B[39mnum_components)\n\u001B[1;32m----> 6\u001B[0m     \u001B[43mgmm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m gmm\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\mixture\\_base.py:200\u001B[0m, in \u001B[0;36mBaseMixture.fit\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    175\u001B[0m     \u001B[38;5;124;03m\"\"\"Estimate model parameters with the EM algorithm.\u001B[39;00m\n\u001B[0;32m    176\u001B[0m \n\u001B[0;32m    177\u001B[0m \u001B[38;5;124;03m    The method fits the model ``n_init`` times and sets the parameters with\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;124;03m        The fitted mixture.\u001B[39;00m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 200\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_predict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    201\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\mixture\\_base.py:230\u001B[0m, in \u001B[0;36mBaseMixture.fit_predict\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    203\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit_predict\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    204\u001B[0m     \u001B[38;5;124;03m\"\"\"Estimate model parameters using X and predict the labels for X.\u001B[39;00m\n\u001B[0;32m    205\u001B[0m \n\u001B[0;32m    206\u001B[0m \u001B[38;5;124;03m    The method fits the model n_init times and sets the parameters with\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    228\u001B[0m \u001B[38;5;124;03m        Component labels.\u001B[39;00m\n\u001B[0;32m    229\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 230\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat64\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mensure_min_samples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    231\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_components:\n\u001B[0;32m    232\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    233\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected n_samples >= n_components \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    234\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got n_components = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_components\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    235\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn_samples = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mX\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    236\u001B[0m         )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\base.py:577\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[0;32m    575\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation should be done on X, y or both.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    576\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[1;32m--> 577\u001B[0m     X \u001B[38;5;241m=\u001B[39m check_array(X, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n\u001B[0;32m    578\u001B[0m     out \u001B[38;5;241m=\u001B[39m X\n\u001B[0;32m    579\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\validation.py:879\u001B[0m, in \u001B[0;36mcheck_array\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[0;32m    877\u001B[0m     \u001B[38;5;66;03m# If input is 1D raise error\u001B[39;00m\n\u001B[0;32m    878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m array\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 879\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    880\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected 2D array, got 1D array instead:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124marray=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    881\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    882\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myour data has a single feature or array.reshape(1, -1) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    883\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mif it contains a single sample.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(array)\n\u001B[0;32m    884\u001B[0m         )\n\u001B[0;32m    886\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dtype_numeric \u001B[38;5;129;01mand\u001B[39;00m array\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mkind \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUSV\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    887\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    888\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnumeric\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is not compatible with arrays of bytes/strings.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    889\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConvert your data to numeric values explicitly instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    890\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# Train MDT model\n",
    "mdt_model = train_mdt_model(features, num_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### *Conclusion:* The model which can be recommended to the client is ... (Put here also essential recommendations to the client how to use your model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4 - Saving the model\n",
    "Save the DS best model in the Jupyter notebook `model.ipynb` in one of the following formats:\n",
    "\n",
    "- `network.save('model.h5')` #keras\n",
    "- `joblib.dump(model, \"model.pkl\")` # optional\n",
    "- `torch.save(model.state_dict(), './model.pt')` #pytorch\n",
    "- `model.save('path/to/model')`\n",
    "\n",
    "End of document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
