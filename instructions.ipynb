{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# How to deliver your Data Science project?\n",
    "\n",
    "#### The following inputs are mandatory:\n",
    "* File #1 `model.ipynb` - main Jupyter notebook should include:\n",
    "    1) use case development according to the given format;\n",
    "    2) clear conclusion which model is the best and can be recommended to the client;\n",
    "    3) *model saving* piece of code.\n",
    "* File #2 `load_predict.py` - file for deployment, put here only saved trained model and predictions on an actual data.\n",
    "Actual data is not a test subsample, it can be a couple of pictures for checking how the model works.\n",
    "* Demo link #3 - up to 1 min demo, screen recording with comments how we should use your model and it makes predictions.\n",
    "\n",
    "# - model.ipynb file structure\n",
    "Please use the template given.\n",
    "\n",
    "- **Name of the use case**\n",
    "        - Overview\n",
    "        - Methodology\n",
    "        - Business Segments\n",
    "        - AI application\n",
    "        - Data\n",
    "        - Papers\n",
    "        - Team\n",
    "- **Data preprocessing, EDA**\n",
    "- **Models**\n",
    "- **Saving the model**\n",
    "\n",
    "# - load_predict.py file structure\n",
    "For deployment purposes, please create a separate `load_predict.py` file for deployment - include there only 1) saved trained model and 2) predictions. Please use the template given.\n",
    "\n",
    "### Please find the example how you can load the model and make predictions.\n",
    "\n",
    "#### Load the model\n",
    "- `model_keras = keras.models.load_model('path/to/model/model.h5')` #keras\n",
    "- `model_pkl = joblib.load(\"path/to/model/model.pkl\")` #optional\n",
    "- `model_pytorch = model.load_state_dict(torch.load('path/to/pytorch/model/model.pt'))` #pytorch\n",
    "\n",
    "#### Predictions using the loaded model\n",
    "- `outputs = model_pkl.predict(input_tensor)` #keras with pkl\n",
    "- `outputs = model_keras.predict(input_tensor)` #keras\n",
    "- `outputs = model_pytorch(input_tensor)` #pytorch\n",
    "\n",
    "####  Decode output tensor if required\n",
    "- `decoded_output = decode(outputs)`\n",
    "- when decode() function - is a custom function that takes the output tensor of the model as input and returns a decoded value that can be read by an outsider. It can be for example a string of a word or a sentence if the task requires it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-27T22:10:44.375372Z",
     "iopub.execute_input": "2023-02-27T22:10:44.375768Z",
     "iopub.status.idle": "2023-02-27T22:10:44.381274Z",
     "shell.execute_reply.started": "2023-02-27T22:10:44.375737Z",
     "shell.execute_reply": "2023-02-27T22:10:44.380199Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the trained model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### This model was saved by this function:\n",
    "If you use pytorch library than you need to use this command:\n",
    "- `torch.save(model.state_dict(), './model.pt')`\n",
    "If you use keras, you must also specify the process of saving and loading models:\n",
    "- `model.save('path/to/model')`\n",
    "`model = keras.models.load_model('path/to/model')`\n",
    "\n",
    "\n",
    "#### Next, we should be able to load the model, so we need to specify this function in the notebook:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "iris_model = nn.Sequential(\n",
    "            nn.Linear(4,32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32,3),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "\n",
    "# iris_model.load_state_dict(torch.load('/kaggle/input/iris-pymodel/iris_model.pt'))\n",
    "iris_model.load_state_dict(torch.load(\"C:\\\\Users\\\\HOME\\\\Zummit_2023\\\\model_template.pt\"))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-22T10:18:59.060916Z",
     "iopub.execute_input": "2023-02-22T10:18:59.061267Z",
     "iopub.status.idle": "2023-02-22T10:18:59.095097Z",
     "shell.execute_reply.started": "2023-02-22T10:18:59.061235Z",
     "shell.execute_reply": "2023-02-22T10:18:59.093781Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Now you need create input tensor. This can be or image tensor, or audio signal, a time series, etc., which is fed to the input of the loaded model. In our simplest case, these are the sizes of iris petals:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (0): Linear(in_features=4, out_features=32, bias=True)\n  (1): Tanh()\n  (2): Linear(in_features=32, out_features=3, bias=True)\n  (3): LogSoftmax(dim=1)\n)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_model.eval() # переводим модель в режим применения"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "input_tensor = torch.tensor([[4.9,2.4,3.3,1.0]])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-22T10:18:59.096733Z",
     "iopub.execute_input": "2023-02-22T10:18:59.097249Z",
     "iopub.status.idle": "2023-02-22T10:18:59.103228Z",
     "shell.execute_reply.started": "2023-02-22T10:18:59.097204Z",
     "shell.execute_reply": "2023-02-22T10:18:59.102344Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Next, we apply the input tensor to the model input and get the output tensor for predictions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "outputs = iris_model(input_tensor)\n",
    "outputs = iris_model(input_tensor)\n",
    "\n",
    "_, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "print('Output Tensor:\\n', outputs)\n",
    "print('\\nTorch Max Function Result: ', predicted)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-22T10:18:59.105276Z",
     "iopub.execute_input": "2023-02-22T10:18:59.105807Z",
     "iopub.status.idle": "2023-02-22T10:18:59.121402Z",
     "shell.execute_reply.started": "2023-02-22T10:18:59.105773Z",
     "shell.execute_reply": "2023-02-22T10:18:59.119948Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Tensor:\n",
      " tensor([[-7.3175e+00, -1.3281e+01, -6.6580e-04]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "\n",
      "Torch Max Function Result:  tensor([2])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, you need to convert the output value of the model into a format that is readable by an outside user of the application. In our case, we need to get the name of the iris in text format based on the model's prediction:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "iris_dict = {0: 'Iris-setosa', 1: 'Iris-virginica', 2: 'Iris-versicolor'}\n",
    "\n",
    "iris_dict[int(predicted)]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-22T10:18:59.123249Z",
     "iopub.execute_input": "2023-02-22T10:18:59.123835Z",
     "iopub.status.idle": "2023-02-22T10:18:59.134530Z",
     "shell.execute_reply.started": "2023-02-22T10:18:59.123800Z",
     "shell.execute_reply": "2023-02-22T10:18:59.133345Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "'Iris-versicolor'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "End of document."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
